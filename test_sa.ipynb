{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/physical_backdoor/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wednesday, 24. January 2024 08:12PM\n",
      "------------------ Currently evaluating gradient-matching ------------------\n",
      "Namespace(f='/home/ubuntu/.local/share/jupyter/runtime/kernel-v2-14119QloVfXHBqbU6.json', net=['ResNet50'], dataset='datasets/Facial_recognition_crop_partial', recipe='gradient-matching', threatmodel='clean-single-source', num_source_classes=1, scenario='finetuning', poisonkey='0-9', system_seed=None, poison_seed=12345, model_seed=23456, deterministic=False, name='', poison_path='poisons/', model_savepath='models/', mixing_method=None, mixing_disable_correction=True, mixing_strength=None, disable_adaptive_attack=True, defend_features_only=False, gradient_noise=None, gradient_clip=None, defense_type=None, defense_strength=None, defense_steps=None, defense_sources=None, defense='', clean_budget=0.2, checkpoints='../../checkpoints/', device='cuda:0', result='./results', defense_set='testset', attack_mode='all2one', nc_scenario='random-poison', temps='./temps', nc_lr=0.1, input_height=224, input_width=224, input_channel=3, init_cost=0.001, atk_succ_threshold=99.0, early_stop=True, early_stop_threshold=99.0, early_stop_patience=10, patience=5, cost_multiplier=2, nc_epoch=60, lr_decay_step=5, lr_decay_factor=0.95, target_label=1, total_label=8, EPSILON=1e-07, to_file=True, n_times_test=1, padversarial=None, pmix=False, attackoptim='signAdam', attackiter=250, init='randn', tau=0.1, scheduling=True, poison_scheduler='cosine', source_criterion='cross-entropy', restarts=1, pbatch=64, paugment=True, data_aug='default', full_data=False, ensemble=1, stagger=False, clean_grad=False, step=False, train_max_epoch=40, ablation=1.0, loss='similarity', centreg=0, normreg=0, repel=0, vruns=3, vnet=None, retrain_from_init=False, skip_clean_training=False, optimization='conservative-sgd', batch_size=32, lr=0.01, noaugment=False, cache_dataset=False, dryrun=False, save=None, save_clean_model=True, exp_name=None, local_rank=None, keep_sources=True, sources_train_rate=1.0, sources_selection_rate=1.0, source_gradient_batch=32, val_max_epoch=40, retrain_max_epoch=30, retrain_scenario=None, load_feature_repr=True, load_trained_model=True, trigger='sunglasses', digital_trigger=False, digital_trigger_path='digital_triggers', opacity=0.12549019607843137, retrain_iter=100, source_selection_strategy=None, poison_selection_strategy='max_gradient', poison_triggered_sample=True, backdoor_finetuning=False, eps=8, alpha=0.0, beta=0.2, poison_triggered_samples=True, exp_namev='test_distance', output='/home/ubuntu/21thinh.dd/Clean-label-Physical-Backdoor-Attack/outputs/gradient-matching/finetuning/sunglasses/RESNET50/0-9_finetuning_sunglasses_0.0_0.2_signAdam_250.txt')\n",
      "CPUs: 32, GPUs: 3 on 126278fe2a2b.\n",
      "GPU : NVIDIA GeForce RTX 3090\n",
      "ResNet50 model initialized with random key 23456.\n",
      "Hyperparameters(name='conservative', epochs=40, batch_size=32, optimizer='SGD', lr=0.0001, scheduler='linear', weight_decay=0.0005, augmentations='default', privacy={'clip': None, 'noise': None, 'distribution': None}, validate=10, novel_defense=None, mixing_method=None, adaptive_attack=True, defend_features_only=False)\n",
      "Normalization disabled.\n",
      "Initializing poison data with random seed 12345\n"
     ]
    }
   ],
   "source": [
    "\"\"\"General interface script to launch poisoning jobs.\"\"\"\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import forest\n",
    "\n",
    "from forest.filtering_defenses import get_defense\n",
    "from forest.utils import write, set_random_seed\n",
    "from forest.consts import BENCHMARK, NUM_CLASSES\n",
    "torch.backends.cudnn.benchmark = BENCHMARK\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3,2,0\"\n",
    "\n",
    "# Parse input arguments\n",
    "args = forest.options().parse_args()\n",
    "args.poisonkey = '0-9'\n",
    "args.trigger = 'sunglasses'\n",
    "args.recipe = 'gradient-matching'\n",
    "args.poison_triggered_samples = True\n",
    "args.alpha = 0.0\n",
    "args.beta = 0.2\n",
    "args.poison_triggered_sample = True\n",
    "args.model_seed = 23456\n",
    "args.poison_seed = 12345\n",
    "args.exp_namev = 'test_distance'\n",
    "args.batch_size = 32\n",
    "args.source_gradient_batch = 32\n",
    "\n",
    "if args.system_seed != None:\n",
    "    set_random_seed(args.system_seed)\n",
    "\n",
    "if args.exp_name is not None:\n",
    "    parent_dir = os.path.join(os.getcwd(), f'outputs_{args.exp_name}')\n",
    "else:\n",
    "    parent_dir = os.path.join(os.getcwd(), 'outputs')\n",
    "\n",
    "if args.defense != '':\n",
    "    args.output = f'{parent_dir}/defense/{args.defense}/{args.recipe}/{args.scenario}/{args.trigger}/{args.net[0].upper()}/{args.poisonkey}_{args.scenario}_{args.trigger}_{args.alpha}_{args.beta}_{args.attackoptim}_{args.attackiter}.txt'\n",
    "else:\n",
    "    args.output = f'{parent_dir}/{args.recipe}/{args.scenario}/{args.trigger}/{args.net[0].upper()}/{args.poisonkey}_{args.scenario}_{args.trigger}_{args.alpha}_{args.beta}_{args.attackoptim}_{args.attackiter}.txt'\n",
    "\n",
    "args.dataset = os.path.join('datasets', args.dataset)\n",
    "\n",
    "os.makedirs(os.path.dirname(args.output), exist_ok=True)\n",
    "open(args.output, 'w').close() # Clear the output files\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "if args.deterministic:\n",
    "    forest.utils.set_deterministic()\n",
    "\n",
    "setup = forest.utils.system_startup(args) # Set up device and torch data type\n",
    "\n",
    "model = forest.Victim(args, num_classes=NUM_CLASSES, setup=setup) # Initialize model and loss_fn\n",
    "data = forest.Kettle(args, model.defs.batch_size, model.defs.augmentations,\n",
    "                        model.defs.mixing_method, setup=setup) # Set up trainloader, validloader, poisonloader, poison_ids, trainset/poisonset/source_testset\n",
    "witch = forest.Witch(args, setup=setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_poisons(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import copy\n",
    "import os\n",
    "from torchvision.transforms import v2\n",
    "from imutils import face_utils\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "\n",
    "class FaceDetector:\n",
    "    def __init__(self, args, dataset=None, patch_trigger=False, constrain_perturbation=False):\n",
    "        \"\"\"\n",
    "        dataset: Poisoned dataset\n",
    "        trigger: Trigger name\n",
    "        \"\"\"\n",
    "        self.device = torch.device('cpu')\n",
    "        self.detector = MTCNN(self.device)\n",
    "        self.landmark_predictor_1 = dlib.shape_predictor('landmarks/shape_predictor_68_face_landmarks.dat')\n",
    "        self.landmark_predictor_2 = dlib.shape_predictor('landmarks/shape_predictor_81_face_landmarks.dat')\n",
    "        self.args = args\n",
    "        self.constrain_perturbation = constrain_perturbation\n",
    "        self.patch_trigger = patch_trigger\n",
    "        \n",
    "        if patch_trigger:\n",
    "            trigger_path = os.path.join('digital_triggers', self.args.trigger)\n",
    "            self.transform = v2.Compose([\n",
    "                        v2.ToImageTensor(),\n",
    "                        v2.ConvertImageDtype(),\n",
    "                    ])\n",
    "            self.trigger_img = np.array(Image.open(trigger_path))\n",
    "            \n",
    "        if dataset is not None:\n",
    "            self.dataset = dataset\n",
    "            self.dataset_landmarks = self.get_dataset_overlays()\n",
    "            \n",
    "        self.indices_lookup = dict(zip(self.poison_target_ids, range(self.poison_num)))\n",
    "\n",
    "    def get_landmarks(self, img):\n",
    "        img_rescale = (img * 255.0).to(torch.uint8).permute(1,2,0).numpy()\n",
    "        x1, y1, x2, y2 = self.detector.detect(img_rescale)[0][0]\n",
    "        facebox = dlib.rectangle(left=int(x1), right=int(x2), top=int(y1), bottom=int(y2))\n",
    "\n",
    "        landmarks_1 = self.landmark_predictor_1(img_rescale, facebox)\n",
    "        landmarks_2 = self.landmark_predictor_2(img_rescale, facebox)\n",
    "        shape_1 = face_utils.shape_to_np(landmarks_1)\n",
    "        shape_2 = face_utils.shape_to_np(landmarks_2)\n",
    "\n",
    "        shape = np.concatenate((shape_1, shape_2[68:]), axis=0)\n",
    "        \n",
    "        return shape\n",
    "    \n",
    "    def get_dataset_overlays(self):\n",
    "        \"\"\"\n",
    "        Given a Dataset object, return a dictionary of landmarks and facial area for each image\n",
    "        \"\"\"\n",
    "        if self.constrain_perturbation:\n",
    "            self.dataset_face_overlay = torch.zeros((len(self.dataset), 224, 224))\n",
    "            \n",
    "        if self.patch_trigger:\n",
    "            self.trigger_mask = torch.zeros((len(self.dataset), 4, 224, 224)) #4 as we have alpha layer\n",
    "        \n",
    "        for img, target, idx in self.dataset:\n",
    "            landmarks = self.get_landmarks(img)\n",
    "            mask = np.zeros((224, 224))\n",
    "            boundaries = np.asarray([landmarks[i] for i in range(len(landmarks)) if i < 27 and i > 67])\n",
    "            mask = torch.tensor(cv2.fillConvexPoly(mask, boundaries, 1)).to(torch.bool)\n",
    "            self.dataset_face_overlay[idx] = mask\n",
    "            if self.trigger != None:\n",
    "                self.trigger_mask[idx] = self.get_transform_trigger(landmarks)           \n",
    "    \n",
    "    def visualize_landmarks(self, img, shape):\n",
    "        img_rgb = img.permute(1,2,0).numpy()\n",
    "        \n",
    "        for idx, (x,y) in enumerate(shape):\n",
    "            cv2.circle(img_rgb, (x, y), 1, (0, 255, 0), -1)\n",
    "            cv2.putText(img_rgb, str(idx), (x+2, y), cv2.FONT_HERSHEY_DUPLEX, 0.2, (0, 255, 0), 1)\n",
    "        \n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.imshow(img_rgb)\n",
    "        \n",
    "    def get_position(self, landmarks):\n",
    "        sun_h, sun_w, _ = self.trigger_img.shape\n",
    "        top_nose = np.asarray([landmarks[27][0], landmarks[27][1]])\n",
    "        if self.args.trigger == 'sunglasses':         \n",
    "            top_left = np.asarray([landmarks[2][0]-5, landmarks[19][1]])\n",
    "            top_right = np.asarray([landmarks[14][0]+5, landmarks[24][1]])\n",
    "            if abs(top_left[0] - top_nose[0]) > abs(top_right[0] - top_nose[0]):\n",
    "                diff = abs(top_left[0] - top_nose[0]) - abs(top_right[0] - top_nose[0])\n",
    "                top_right[0] = min(top_right[0] + diff // 2, 223)\n",
    "                top_left[0] += diff // 2\n",
    "            else:\n",
    "                diff = abs(top_right[0] - top_nose[0]) - abs(top_left[0] - top_nose[0])\n",
    "                top_right[0] -= diff // 2\n",
    "                top_left[0] = min(top_left[0] - diff // 2, 223)\n",
    "            \n",
    "            # calculate new width and height, moving distance for adjusting sunglasses\n",
    "            width = np.linalg.norm(top_left - top_right)\n",
    "            scale = width / sun_w\n",
    "            height = int(sun_h * scale)\n",
    "            \n",
    "        elif self.trigger == 'white_facemask':\n",
    "            top_left = np.asarray([landmarks[0][0], landmarks[28][1]])\n",
    "            top_right = np.asarray([landmarks[16][0], landmarks[28][1]])\n",
    "            height = abs(landmarks[8][1] - landmarks[0][1]) # For facemask\n",
    "            \n",
    "        elif self.trigger == 'red_headband':\n",
    "            top_left = np.asarray([landmarks[0][0] - 5, landmarks[69][1]])\n",
    "            top_right = np.asarray([landmarks[16][0] + 5, landmarks[72][1]])\n",
    "            \n",
    "            width = np.linalg.norm(top_left - top_right)\n",
    "            scale = width / sun_w\n",
    "            height = abs(landmarks[72][1] - landmarks[19][1])\n",
    "\n",
    "        unit = (top_left - top_right) / np.linalg.norm(top_left - top_right)\n",
    "\n",
    "        perpendicular_unit = np.asarray([unit[1], -unit[0]])\n",
    "\n",
    "        bottom_left = top_left + perpendicular_unit * height\n",
    "        bottom_right = bottom_left + (top_right - top_left)\n",
    "        \n",
    "        return top_left, top_right, bottom_right, bottom_left\n",
    "    \n",
    "    def get_transform_trigger(self, landmarks):\n",
    "        \"\"\"\n",
    "        img: Torch tensor, (3, H, W)\n",
    "        trigger: Torch tensor, (3, H, W)\n",
    "        \"\"\"        \n",
    "        top_left, top_right, bottom_right, bottom_left = self.get_position(landmarks)\n",
    "\n",
    "        dst_points = np.asarray([\n",
    "                top_left, \n",
    "                top_right,\n",
    "                bottom_right,\n",
    "                bottom_left], dtype=np.float32)\n",
    "\n",
    "        src_points = np.asarray([\n",
    "            [0, 0],\n",
    "            [self.trigger_img.shape[1] - 1, 0],\n",
    "            [self.trigger_img.shape[1] - 1, self.trigger_img.shape[0] - 1],\n",
    "            [0, self.trigger_img.shape[0] - 1]], dtype=np.float32)\n",
    "\n",
    "        M, _ = cv2.findHomography(src_points, dst_points)\n",
    "        transformed_trigger = cv2.warpPerspective(self.trigger_img, M, (224, 224), None, cv2.INTER_LINEAR, cv2.BORDER_CONSTANT)\n",
    "        return self.transform(transformed_trigger)\n",
    "    \n",
    "    def lookup_poison_indices(self, image_ids):\n",
    "        \"\"\"Given a list of image_ids, retrieve the appropriate indices for facial masks and trigger masks\n",
    "        Return:\n",
    "            indices: indices in the trigger masks and facial masks\n",
    "        \"\"\"\n",
    "        indices = []\n",
    "        for image_id in image_ids:\n",
    "            lookup = self.indices_lookup.get(image_id)\n",
    "            indices.append(lookup)\n",
    "\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "    \n",
    "    def patch_inputs(self, inputs, poison_indices):\n",
    "        \"\"\"Patch the inputs\n",
    "        Args:\n",
    "            inputs: Torch.tensor[batch, 3, 224, 224]: Batch of inputs to be patched\n",
    "            indices: Torch.tensor Image ids from training data.\n",
    "        \"\"\"\n",
    "        alpha_trigger_masks = self.trigger_mask[poison_indices, 3, ...].bool() * self.args.opacity # [N, 224, 224] mask\n",
    "        alpha_inputs_masks = 1.0 - alpha_trigger_masks\n",
    "        for depth in range(0, 3):  \n",
    "            inputs[poison_indices, depth, ...] =  (\n",
    "                inputs[poison_indices, depth, ...] * alpha_inputs_masks + \n",
    "                self.trigger_mask[poison_indices, depth, ...]  * alpha_trigger_masks\n",
    "            )\n",
    "    \n",
    "    def get_face_overlays(self, poison_indices):\n",
    "        \"\"\"Given a list of image_ids, retrieve the appropriate indices for facial masks and trigger masks\n",
    "        Return:\n",
    "            indices: indices in the trigger masks and facial masks\n",
    "        \"\"\"\n",
    "        return self.dataset_face_overlay[poison_indices]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FaceDetector' object has no attribute 'trigger'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m face_detector \u001b[38;5;241m=\u001b[39m \u001b[43mFaceDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoisonset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstrain_perturbation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m, in \u001b[0;36mFaceDetector.__init__\u001b[0;34m(self, args, dataset, patch_trigger, constrain_perturbation)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m dataset\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_landmarks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataset_overlays\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices_lookup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoison_target_ids, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoison_num)))\n",
      "Cell \u001b[0;32mIn[5], line 61\u001b[0m, in \u001b[0;36mFaceDetector.get_dataset_overlays\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstrain_perturbation:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_face_overlay \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset), \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrigger_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset), \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)) \u001b[38;5;66;03m#4 as we have alpha layer\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, target, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FaceDetector' object has no attribute 'trigger'"
     ]
    }
   ],
   "source": [
    "face_detector = FaceDetector(args, dataset=data.poisonset, constrain_perturbation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "if args.skip_clean_training:\n",
    "    write('Skipping clean training...', args.output)\n",
    "else:\n",
    "    model.train(data, max_epoch=args.train_max_epoch)\n",
    "train_time = time.time()\n",
    "\n",
    "print(\"Train time: \", str(datetime.timedelta(seconds=train_time - start_time)))\n",
    "\n",
    "# Select poisons based on maximum gradient norm\n",
    "data.select_poisons(model)\n",
    "\n",
    "# Print data status\n",
    "data.print_status()\n",
    "\n",
    "poison_delta = witch.brew(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physical_backdoor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
